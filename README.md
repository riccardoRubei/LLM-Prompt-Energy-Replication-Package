# Energy-Aware Prompt Engineering for Large Language Models: An Empirical Study on Software Engineering Tasks
Replication Package for the paper: "Energy-Aware Prompt Engineering for Large Language Models: An Empirical
Study on Software Engineering Tasks"

#### Overview of the package
- Analysis: This folder contains the scripts we used to perform the analysis. The scripts are divided by task (Code Completion and Text Summarization)
- Data: This folder contains all the results we obtained from our study. See the next section for further details.
- Prompt Snippets: This folder contains the scripts we used to query Llama3.1-Instruct and Codellama-Instruct.

#### Description of the Data Folder
**Code Completion**
- The folder contains two folders (Llama3.1 and Codellama) and the Updated_Test.json which contains the data from CodeXGlue.
- The two folders contain our results obtained from the execution of the Llama3.1-Instruct and Codellama inference. Since each sub-folder contains 30k files, we compressed the files.
- Each folder contains 5000 files: 1000 snippets x 5 custom configurations (conf0-conf4).

**Text Summarization**
- The folder contains two folders (Llama3.1 and Codellama) and the file train.csv (compressed) which contains the data from GitSum (https://github.com/MDEGroup/GitSum)
- The two folders contain our results obtained from the execution of the Llama3.1-Instruct and Codellama inference. Since each sub-folder contains about 12k files, we compressed the files.
- Each folder contains 1372 files: 343 README.MD x 4 custom configurations (conf0-conf3).

**Common Structure of the Results Folders**
- Answers: This folder contains all the answer we obtained from the study.
- CodeCarbon: This folder contains the resulst produced by CodeCarbon. Please refer to the official documentation of CodeCarbon for more details. https://mlco2.github.io/codecarbon/output.html
- Questions: This folder contains all the question in the string format we used to conduct the inference.
- Inside a Question and Answer snippet there are 5 questions and answers (each inference has been repeated 5 times) separeted by strings like *>>>Start LLM Answer<<<* to help the user in a visual inspection.
- The CodeCarbon contains the .csv files generated by CodeCarbon. Each file contains 5 rows (one per repeated inference test).


## Requirements
- To replicate the analysis there are not specific infrastructures. The scripts are extremely simple and totaly editable.
- We tested the scripts with Python 3.10.16 and 3.13 (Although we do not recommend the version .13 due to known compatibility issues)
- We reports the Python libraries here to facilitate the decision on what install
    - **Code Carbon Analysis**
        - pandas
    - **Code Completion Answer Analysis**
        - pandas
        - nltk
        - json
    - **Text Summarization Answer Analysis**
        - pandas
        - csv
        - nltk
        - evaluate
        - rouge-score
        - bert-score
- As a general suggestion, we recommend to create a clean virtual enviroment.

## Execution of The Analysis
The execution is extremely simple, just go inside the analysis folder, open a terminal and use python plus some arguments.
For all the analysis we included a default configuration, in case of no arguments.

**Code Carbon Analysis**:
```sh
python Analyze_CodeCarbon.py ARG1
ARG1 = Llama | Codellama
Example: python Analyze_CodeCarbon.py Llama
```

**Code Completion Answer Analysis**:
```sh
python Analyze_Answers.py ARG1 ARG2 ARG3
ARG1 = Llama | Codellama
ARG2 = ZeroShot | OneShot | FewShots
ARG3 = Exact | Edit | Length
Example: python Analyze_Answers.py Llama ZeroShot Edit
```

**Text Summarization Answer Analysis**:
```sh
python Analyze_Text.py ARG1 ARG2 ARG3
ARG1 = Llama | Codellama
ARG2 = ZeroShot | OneShot | FewShots
ARG3 = Rouge | RougeL | Bleu | Meteor | Bert | Length
Example 1: python Analyze_Text.py Llama ZeroShot Rouge
Example 2: python Analyze_Text.py Codellama OneShot Bert
```
